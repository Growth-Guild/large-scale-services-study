# 1주차
- - -
## 강의 4
- 레코드 수가 수천만 단위를 넘어가는 데이터 크기는 수 GB ~ 수백GB정도가 될 수 있다.
- 이 정도의 데이터는 처리하는 데에 시간이 걸리게 되고 아무 생각없이 작성한 쿼리에 대해 응답하지 않는다. 
- 이러한 문제들이 발생하면 데이터 분산 / 인덱스 설정 등으로 해결할 수 있는데 중요한 점은 이러한 기본적인 부분을 바탕으로 ***문제가 발생하면 그 자리에서 생각하자*** 는 점이다.
- - -
## 강의 5

### 대규모 데이터의 어려운 점
- 메모리 내에서 계산할 수 없다.
  - 이로 인해 디스크에 있는 데이터를 검색하게 된다.
- 디스크는 상당히 느리므로 I/O에 시간이 걸린다. 
- 메모리는 디스크보다 10^5 ~ 10^6 배 이상 빠르다.

### 디스크는 왜 느린가
- 디스크는 물리적인 동작을 수반한다.
  - 이 물리적인 구조가 탐색 속도에 영향을 주며 아무리 개선해도 메모리보다 빠를 수 없다.
- 그렇다면 메모리는 왜 빠를까?
  - 물리적인 동작 없이 실제 데이터를 탐색하기에 오버헤드가 거의 없다.
  - 메모리는 1회 탐색 시에 마이크로초로 처리가 가능하지만 디스크는 밀리초가 걸릴 수 있다.

### OS 레벨에서 디스크 단점 개선
- OS는 연속된 데이터를 같은 위치에 쌓고 데이터를 읽을 때 4KB 정도씩 한번에 읽을 수 있다.
- 이 덕분에, 디스크의 회전횟수를 최소화할 수 있지만 결국 회전 1회당 밀리초 단위이므로 메모리와의 속도차를 피할 수는 없다.

### 전송속도, 버스의 속도차
- 디스크와 메모리는 탐색속도 뿐만 아니라 전송속도에서도 많은 차이가 있다.
- 메모리 디스크 모두 CPU와 버스로 연결되어 있다.
- 메모리는 CPU와 상당히 빠른 버스로 연결되어 있지만 디스크는 그렇지 않기에 데이터가 많아지면 많아질 수록 전송 속도의 격차가 점점 벌어질 수 있다.
  - ex) `hdparm`이라는 Linux 툴로 측정 가능
- SSD가 나오면서 탐색속도에 개선이 점점 되고는 있지만, 버스 속도가 병목이 되거나 그 밖의 구조에 의지하기에 메모리만큼의 속도는 나오지 않는다.

***개발자는 확장성을 고려하여 메모리와 디스크 속도차를 생각하고 애플리 케이션을 만들어야 한다. 이는 매우 본질적이면서도 어려운 부분이다.***

### Linux 단일 호스트의 부하
- 서버 한대로 처리가능한 부하를 여러 서버로 분산하는 것은 `분산`에 어울리지 않다.
- 단일 서버의 성능을 충분히 끌어낼 수 있는 것을 시작으로 복수 서버에서의 분산이 의미를 갖는다.

#### 부하 계측방법

1. **Load Average 확인**
    - Load Average 는 시스템 전체의 부하상황을 나타내는 지표이다. 
    - top 이나 uptime 등의 명령으로 Load Average 를 확인할 수 있다.
2. **CPU, I/O 중 병목 원인 조사**
    - Load Average 가 높은 경우, CPU 와 I/O 어느 쪽에 원인이 있는지를 조사한다.
    - sar 이나 vmstat 로 시간 경과에 따라 CPU 사용률이나 I/O 대기율의 추이를 확인할 수 있으므로 이를 참고하여 규명한다.

- 일반적으로 CPU에 부하가 걸리는 상황들이다.
  1. 프로그램의 폭주
      - 프로그램의 개선 / 메모리 증설 혹은 분산을 검토해볼 수 있다.
  2. 디스크 혹은 메모리 용량(트래픽 부하 등으로 인한) 등을 제외한 부분에서는 병목이 되지 않는 상황
      - 이는 서버 증설 혹은 프로그램의 로직 / 알고리즘의 개선으로 대응할 수 있다.

#### OS튜닝이란
- 부하를 계측했다면, 이제 OS 성능을 향상시키기 위한 튜닝을 할 수 있다.
- 본래 튜닝의 의미는 ***병목의 제거*** 작업과 같다. 즉, 문제가 될만한 부분을 제거하는 것이다.
- 예를 들면 I/O 성능을 개선하기 위해 이와 같은 작업들을 고려할 수 있다.
  - 메모리 증설을 통해 캐시 영역을 확보하여 대응할 수 있는가
  - 원래 데이터량이 너무 많지 않은가
  - 애플리케이션의 I/O 알고리즘을 변경할 필요가 있는가

***원인을 알면 대응방법은 자명하기에, 이를 실천하는 것이 `튜닝`인 것이다***
- - -
## 강의 6

### 규모조정, 확장성
- 대규모 환경이란, 여러 서버를 통해 부하를 분산하는 환경이라 해도 무방하다.
  - 웹 서비스에서는 서버자체의 스펙을 올리는 `scale-up`보다는 여러 개의 저스펙의 서버를 나열해 시스템 전체 성능을 올리는 `scale-out` 전략이 주류를 이룬다.
  - 10배의 과금이 발생하는 서버의 성능이 속도나 신뢰성 면에서 10배만큼 발휘하지 않기 때문이다.
- 시스템 구성의 유연성이란, 공통적으로 상황 대처가 쉽다는 점이다.
  - 부하가 적은 경우엔, 최소한의 투자로 부하가 높아짐에 따라 확장하기 쉽다는 점
  - 상당한 용도의 서버도 저렴하고 간단하게 준비할 수 있다는 점

`규모조정의 요소`
- `sclae-out`은 서버를 횡으로 전개해 확장성을 확보하게 되는데, 이 때 CPU 부하의 확장성을 확보하기는 쉽다.
  - ex) http요청을 받아 db로부터 받은 데이터를 클라이언트로 응답할 때는 기본적으로 cpu부하만 소요
- 하지만 DB측면에서는 I/O 부하만 걸린다.
  - 여러 DB를 두었을 때(ex) slave DB, master DB) 데이터에 변화가 생길 때, 동기화하는 부분에서 문제가 생길 수 있다. 이처럼 쓰기는 간단히 분산할 수가 없다.
  - 즉, 대규모 환경에서는 I/O 부하를 부담하고 있는 서버는 분산시키기 어렵고 디스크 I/O가 많이 발생하면 서버가 금새 느려지는 문제가 있다.

***이처럼 규모조정 요소에서 CPU부하 뿐만 아니라, I/O부하에 대해 생각해야 한다는 점을 확실히 파악해둬야 한다.***

`멀티태스킹 OS와 부하`
- Windows, Linux 등 멀티태스킹이 가능한 OS들은, 짧은 시간 간격으로 여러 태스크를 전환해가며 처리함으로써 멀티태스킹을 실현하고 있다.
- 실행할 태스크가 적으면 대기없이 전환이 가능하지만, 태스크가 늘어날 수록 수행 중인 특정 태스크가 존재하는 동안 나머지 태크스들은 CPU에 시간이 날 때까지 대기하게 된다. 이는 프로그램의 `실행지연`으로 나타나게 된다.
  - top의 출력내용에는 `Load Average`라는 수치가 포함되어 있는데, 평균적으로 어느 정도의 태스크가 대기상태로 있엇는지를 보고하는 수치다. 따라서, 이 수치가 높을만큼 부하가 높은 상황이라고 할 수 있다.
  - 하드웨어는 일정 주기로 CPU로 인터럽트 신호를 보낸다.(=타이머 인터럽트)
  - 이 일정 주기 동안 실행 중인 프로세스의 CPU사용량 등의 시간관련 처리를 수행한다.
  - 이 주기마다 Load Average 값이 계산된다.
  - 커널은 타이머 인터럽트가 발생했을 때 실행가능 태스크와 I/O 대기인 태스크의 실행개수를 세어둔다. 그 값을 단위 시간으로 나눈 것이 Load Average 값이라 할 수 있다.
    - (실행가능 태스크 = 다른 태스크가 CPU를 점유하고 있어 계산을 시작할 수 없는 태스크)
  - 즉, Load Average가 의미하는 부하는 이와 같은 의미를 갖고 있다.
    - 처리를 실행하려 해도 할 수 없어서 대기하고 있는 프로세스의 수
      - CPU의 실행권한이 부여되기를 기다리고 있는 프로세스
      - 디스크 I/O가 완료하기를 기다리고 있는 프로세스
  - 하지만, Load Average 값만으로는 CPU 부하가 높은지, I/O 부하가 높은지 판단할 수 없다.
- - -
## 강의 7

### 대규모 데이터를 다루는 세 가지 급소  - `프로그램 작성 요령`
- 어떻게 하면 메모리에서 처리를 마칠 수 있을까?
  - 디스크 seek 횟수를 최소화
    - 디스크 seek 횟수가 확장성, 성능에 크게 영향을 주기 때문이다.
- 데이터량 증가에 강한 알고리즘 사용
  - ex) 선형검색 -> 이분검색 (O(n) -> O(logn))
- 데이터 압축 혹은 검색기술 활용
  - 데이터를 압축하면 디스크 seek 횟수가 적어지며 메모리에 캐싱하기 쉬워진다.
  - 검색이 중요한 이유는 확장성 면에서 DB에만 의존해 해결하지 못할 때, 특정 용도에 특화된 검색엔진 등을 만들어 해당 검색시스템을 웹 어플리케이션에서 이용하는 형태로 전환한다면 속도를 제대로 확보할 수 있다.

- - - 
## 강의 8

### 대규모 데이터를 다루기 전, 3대 전제지식
- `OS캐시`
  - I/O 대책에 대한 기반은 OS에 있다.
  - OS는 메모리를 이용해서 디스크 액세스를 줄인다.
  - Linux의 경우는 페이지 캐시(= 버퍼 캐시)의 구조를 갖추고 있다.
      
  - `가상 메모리 구조`
    - 가상 메모리 구조란 논리적인 선형 어드레스를 물리적 어드레스로 변환하는 것이다.
    - 가상 메모리 구조가 존재하는 가장 큰 이유는 물리적인 하드웨어를 OS에서 추상화하기 위해서이다.
    - 프로세스에서 메모리를 필요로 하게 되면 OS가 메모리에서 비어있는 곳을 찾는다.
    - 메모리는 OS가 관리하며 비어있는 곳을 반환할 때 다른 어드레스 형태를 반환한다.
      - 프로세스는 메모리의 어느 부분을 사용하는지 관여하지 않는다. 특정 번지부터 시작한다고 정해져 있는 편이 다루기 쉽기 때문이다. (시작 주소가 다르면 메모리를 확보할 주소 위치를 찾기가 매우 어려울 수 있다.)
      
    - ***OS는 메모리를 직접 프로세스로 넘기는 것이 아니라 일단 커널 내에서 메모리를 추상화 하는데 이러한 구조를 가진 것을 가상 메모리 구조라 한다.***
    - 여기서 `페이지`란, OS가 프로세스에게 어드레스를 넘길 때, 1바이트 씩이 아닌 4KB 정도를 하나의 블록으로 확보해 넘기는데, 여기서 이 1개의 블록을 의미한다.

  - `Linux의 페이지 캐시 원리`
    - OS는 확보한 페이지를 메모리상에 계속 확보해둔다.
    - 프로세스는 직접 디스크에 액세스할 수 없기에 받은 페이지를 한번은 메모리상에 위치시켜야 한다. 따라서 OS는 읽어낸 블록을 메모리에 작성한다.
    - 그 후, OS는 메모리 주소를 프로세스에게 가상 어드레스로서 알려주고 프로세스가 해당 메모리에 액세스할 수 있게 된다.
    - 데이터 읽기를 마친 프로세스는 할당된 메모리를 해제하지 않고 남겨둠으로써 후에 디스크를 읽으러 갈 필요 없이 데이터를 메모리에서 바로 읽을 수 있게 된다. = ***`페이지 캐시`***
    - 즉, Linux에서는 디스크에 데이터를 읽으러 가면 꼭 한번은 메모리에 캐싱한다. 이후 두번째 부터는 액세스가 빨라질 수 있다.
    - OS를 가동하는 동안에 메모리가 허락하는 한, 디스크상의 데이터를 계속 캐싱할 수 있게 된다.

  - `VFS`
    - 실제 디스크를 조작하는 드라이버와 OS 사이에는 파일 시스템이 존재한다.
    - 파일 시스템 하위에 드라이버가 존재하며, 이 드라이버가 실제로 하드디스크 등을 조작한다.
    - 파일 시스템 위에는 `VFS`라는 가상 파일시스템이 존재하는데 추상화 레이어이고 페이지 캐시 구조를 지니고 있다.
    - 파일시스템의 다양한 함수의 인터페이스를 통일시키며 이를 통해 어떤 디스크를 읽어내든 동일한 구조로 캐싱된다.
  
  - 페이지 캐시는 투과적으로 작용한다. 특정 파일을 읽으면 쭉 캐싱하기 때문에 갑자기 메모리 사용량이 증가할 수 있다.
- - -
## 강의 9

### I/O 부하를 줄이는 방법
1. 데이터 규모에 비해 물리 메모리가 크면 전부 캐싱할 수 있다.
    - 이에 데이터 압축을 이용하면 디스크 내용을 전부 그대로 캐싱해둘 수도 있다.
2. 경제적 비용과의 밸런스를 고려한다.
    - 현재 일반적인 서버의 메모리가 8~16GB인 것을 고려해 계획을 수립한다.

3. 캐시로 해결될 수 없는 규모인 경우에는 복수 서버로 확장시킬 수 있다.
    - 복수 서버로 확장시키는 것은 부하 때문 만이 아니라,  
    캐시 용량을 늘리고자 할 때, 혹은 효율을 높이고자 할 때도 많다.
    - 하지만 단순히 대수만 늘려서는 확장성을 확보할 수 없다. 데이터를 복사하여 늘리면 애초에 캐시 용량이 부족한 상태 그대로 복사되기 때문에 부족한 부분도 그대로 늘려가게 되는 것이다.  
    이는 곧 다시 병목현상이 발생할 수 있다.
    - - -












