# 1주차
- - -
## 강의 4 - 하테나 북마크의 데이터 규모
* 데이터의 규모는 커질 수록 쿼리를 처리하는데 많은 시간이 소요되므로 인덱스를 잘 걸어주어야 한다.
* 데이터의 규모가 너무 거대하여 인덱스를 통해서도 비기능적 요건이 충족되지 않으면 콜드 데이터를 이관하여 분리 보관 하거나, 샤딩을 고려해봐도 좋을 것 같다.
* 크고 작은 문제들에 대한 해결법을 잘 알고 있는 것은 그만큼 많은 트러블을 접해왔다는 것이다. 문제가 발생하면 시행착오를 거치며 노련하게 경험을 쌓자.

- - -
## 강의 5 - 대규모 데이터 처리의 어려운 점

### 대규모 데이터의 어려운 점
* 메모리 내에서 계산할 수 없기 때문에 지속적으로 디스크에서 읽어야 한다.
* 디스크는 메모리에 비해서 상당히 느리다.
* 메모리는 디스크보다 10^5 ~ 10^6 배 이상 빠르다.

### 디스크는 왜 느린가
* 메모리는 전기적인 부품이므로 물리적 구조는 탐색속도와 그다지 관계가 없다.
* 디스크는 동축 상에 원반에 쌓여있고, 데이터를 읽기 위해서는 메모리와 달리 회전 등의 물리적인 동작을 수반한다. 이러한 물리적인 구조가 탐색 속도에 영향을 준다.

### OS 레벨에서 속도 차이를 줄이는 방법
* 디스크는 속도가 느리지만 OS는 이러한 점을 커버하기 위해 연속된 데이터를 같은 위치에 쌓는다.
* 같은 위치에 쌓인 데이터들은 읽을 때 한꺼번에 읽도록 하여 1번의 디스크 회전만으로 읽는 데이터 수를 늘려서 회전 수를 최소화할 수 있다.
* 그렇다고해서 메모리와의 속도차를 극복할 수 있는 수준은 아니다.

### 전송속도, 버스의 속도차
* 데이터를 읽고나면 어딘가로 전송을 하게 되는데, 이 전송하는 구간을 버스라고 한다.
* 전송속도는 디스크에서 메모리로 보내거나 메모리에서 CPU로 보내는 등 컴퓨터 내부에서 전송하기 위한 속도이다.
* hdparm 이라는 Linux 툴을 사용하면 그 속도차를 알 수 있는데, "Timing cached reads" 는 메모리에 있는 캐시 데이터의 전송속도를 나타내고, "Timing buffered disk reads" 는 디스크의 전송속도이다.
* 메모리와 CPU는 상당히 빠른 버스로 연결되어 있고, 디스크는 상대적으로 속도가 느리다.

### 병목을 구명하기위한 작업

#### Load Average 확인
* Load Average 는 시스템 전체의 부하상황을 나타내는 지표이다. 
* top 이나 uptime 등의 명령으로 Load Average 를 확인할 수 있다.

#### CPU, I/O 중 병목 원인 조사
* Load Average 가 높은 경우, CPU 와 I/O 어느 쪽에 원인이 있는지를 조사한다.
* sar 이나 vmstat 로 시간 경과에 따라 CPU 사용률이나 I/O 대기율의 추이를 확인할 수 있으므로 이를 참고하여 규명한다.

- - -
## 강의 6 - 규모 조정의 요소

### 규모조정, 확장성
* 웹서비스에서는 고가의 빠른 하드웨어로 교체하여 성능을 끌어올리는 스케일업(Scale-up) 전략보다는 저가이면서 보통 성능의 하드웨어를 여러 개 사영하여 시스템의 전체 성능을 올리는 스케일 아웃(Scale-out) 전략을 많이 사용한다.
* 스케일 아웃은 비용이 저렴하고 시스템 구성에 유연성이 있다는 장점이 있다.
* 하드웨어의 가격이 10배 비싸다고 성능도 10배로 좋은 것은 아니므로 적절한 비용을 산정하여 서버를 구축해야한다.
  * (클라우드 서비스를 사용할 수 있는 환경이라면 그게 제일 좋은 것 같다..)

### 규모조정의 요소
* 애플리케이션 서버에서는 CPU 부하만 소요되고, DB 서버 측면에서는 I/O 부하가 걸린다.
  * 단편적으로는 어느정도 맞는 말이지만, 블로킹 방식의 애플리케이션 서버에서는 CPU 부하 뿐만 아니라 대외 서버, DB 로부터 응답을 기다릴 때 네트워크 I/O 도 발생한다.
* 애플리케이션 서버는 CPU 부하만 발생하므로 데이터를 분산하여 가지고 있는 것이 아니기 때문에 수평 확장이 용이하다.
* 데이터베이스는 데이터를 Read/Write 하므로 수평 확장하여 분산 저장을 하면 두 데이터베이스간에 데이터 동기화 이슈가 발생한다.
  * Read 는 Master-Slave 구조로 복제를 하면 되지만, 멀티소스로 두 곳에서 동시에 Write 하는 경우에는 조금 더 복잡한 토폴로지를 구성해야한다.
  * (역시 클라우드를 쓸 수 있다면 편리하게 클라우드를 사용하자!)

### DB 확장성 확보의 어려움
* DB 에서 많은 부하가 발생하면 디스크를 읽고 쓰기가 느린 특성상 병목이 발생할 수 있고, 이런 상황에서는 애플리케이션 서버를 늘리기만 한다고해서 해결할 수 없다.
* 캐시를 적절하게 이용하면 이에 대한 부담을 줄일 수 있다.
* 하지만 캐시를 이용한다면 캐시의 신선도와 최신화된 DB 데이터와 동기화하는 이슈가 발생하므로 적절하게 Evict 하는 정책을 세워야한다.

### 멀티태스킹 OS 와 부하
* 멀티태스킹 환경에서 여러 태스크를 유한한 하드웨어로 처리하는데, 태스크의 수가 많아지면 매우 짧은 시간 간격으로 여러 태스크들을 전환해가며 처리한다.
  * 태스크들을 전환해가며 처리하는 것을 컨텍스트 스위칭이라고 하며, 스위칭이 빈번해지면 대기 시간이 그만큼 증가한다.
* top 명령어의 출력 내용에는 Load Average(평균 부하) 라는 수치가 포함되어 있다.
  * ex) load average: 0.70, 0.66, 0.59
  * 왼쪽부터 차례대로 1분, 5분, 15분 동안에 단위 시간당 대기된 태스크의 수를 나타낸다. 즉, 평균적으로 어느 저옫의 태스크가 대기 상태로 있었는지를 보고하는 수치이다.

### Average 가 보고하는 부하의 정체
* 하드웨어는 일정 주기로 CPU 로 인터럽트(interrupt)라고 하는 신호를 보낸다. 주기적으로 보내지는 신호라는 점에서 타이머 인터럽트(Timer Interrupt) 라고 한다.
* 이 인터럽트마다 CPU 는 시간을 진행시키거나 실행 중인 프로세스가 CPU 를 얼마나 사용했는지를 계산하는 등 시간에 관련된 처리를 수행한다. 이때 타이머 인터럽트마다 Load Average 가 계산된다.

- - -
## 강의 7 - 대규모 데이터를 다루기 위한 기초지식

### 대규모 데이터를 다루는 세 가지 급소
* 대규모 시스템을 고민하게 만드는 대규모 데이터는 '어떻게 하면 메모리에서 처리를 마칠 수 있을까?' 라는 점이다.
* 디스크 seek 횟수가 확장성, 성능에 크게 영향을 미치므로 메모리에서 처리를 마치는 것이 좋다.
* 데이터 증가량에 따른 적절한 알고리즘을 선택하는 것도 방법이다.
* 데이터 압축이나 검색 기술과 같은 테크닉을 활용하는 것도 방법이다.
    * 데이터를 압축하여 데이터량을 줄여낼 수 있다면 읽어내는 seek 횟수도 적어지게 되므로 디스크 읽는 횟수를 최소화 할 수 있다.
* 검색 기술이 중요한 이유는 확장성 면에서 DB 에만 맡겨서 해결할 수 없을 때, 특정 용도에 특화된 검색엔진을 만들어서 해당 검색 시스템을 웹 애플리케이션에서 이용하는 형태로 전환한다면 속도를 확보할 수 있기 때문이다.

- - -
## 강의 8 - OS 의 캐시 구조

### OS 의 캐시 구조를 알고 애플리케이션 작성하기
* OS 는 메모리를 이용해서 디스크 액세스를 줄인다.
* Linux 의 경우는 페이지 캐시(page cache)나 파일 캐시(file cache), 버퍼 캐시(buffer cache)라고 하는 캐시 구조를 갖추고 있다.

### 가상 메모리 구조
* 가상 메모리 구조는 논리적인 선형 어드레스를 물리적인 물리 어드레스로 변환하는 구조이다.
* 가상 메모리는 물리적인 하드웨어를 OS 에서 추상화한다.
* 페이지는 OS 가 물리 메모리를 확보/관리하는 단위이다.
* 페이지는 메모리를 1바이트씩 읽어서 액세스 하는 것이 아니라 4KB 정도의 블록으로 확보하여 읽도록 한다.
* OS 는 프로세스에서 메모리를 요청받으면 페이지를 1개 이상, 필요한 만큼 확보해서 프로세스에 넘기는 작업을 수행한다.

### Linux 의 페이지 캐시 원리
* OS 는 확보한 페이지를 메모리 상에 계속 확보해두는 기능을 가지고 있다. 그 순서는 아래와 같다.
1. OS는 디스크로부터 4KB 크기의 블록을 읽어낸다.
2. OS는 읽어낸 블록을 메모리 상에 위치시키는데, 프로세스는 디스크에 직접 액세스 할 수 없기 때문이다. 프로세스가 액세스 할 수 있는 것은 (가상) 메모리다.
3. OS는 그 메모리 주소를 프로세스에 가상 주소로 알려준다.
4. 프로세스는 해당 메모리에 액세스하게 된다.
5. 데이터 읽기를 마친 프로세스가 데이터는 전부 처리하여 불필요하게 되었어도 메모리를 해제하지 않고 남겨둔다. 이렇게 하면 다음에 다른 프로세스가 같은 디스크에 액세스 할 때 남겨두었던 페이지를 사용할 수 있으므로 디스스크를 읽으러 갈 필요가 없게 된다.

#### 페이지 캐시의 친숙한 효과
* Linux 에서는 디스크에 데이터를 읽으러 가면 꼭 한 번은 메모리로 가서 데이터가 반드시 캐싱된다. 따라서 두 번째 이후의 액세스가 빨라진다.
* 현대의 OS는 대체로 페이지 캐시와 비슷한 구조를 갖추고 있다. 따라서 OS를 계속 가동시켜 두면 빨라진다.

### VFS
* 디스크를 조작하는 디바이스 드라이버와 OS 사이에는 파일시스템이 존재한다.
* Linux 에는 ext3, ext2, ext4, xfs 등 몇몇 파일시스템이 있는데 그 하위에 디바이스 드라이버가 있다. 이 디바이스 드라이버가 실제로 하드디스크를 조작한다.
* 파일 시스템 위에는 VFS(Virtual File System)이라는 추상화 레이어가 있다..
* VFS 는 다양한 함수를 통일 시킬 수 있도록 인터페이스 역할을 하며, 페이지 캐시 구조를 지니고 있다.
    * 어떤 파일 시스템을 읽어내더라도 이 인터페이스 덕분에 동일한 구조로 캐싱된다.

### Linux 는 페이지 단위로 디스크를 캐싱한다.
* OS 는 읽어낸 블록 단위만으로 캐싱할 수 있는 범위가 정해진다. 디스크 상에서 읽어낸 블록만 캐싱하므로 특정 파일의 일부분만 캐싱하게 된다.
* 이러한 특성 덕분에 상대적으로 용량이 큰 디스크에 대해서 적은 메모리로도 캐싱이 가능하다.
* 페이지 = 가상 메모리의 최소 단위

### LRU (Least Recently Used)
* 메모리 여유분이 1.5GB 이고 파일을 4GB 전부 읽게 되면 구조상으로는 LRU 알고리즘을 통해 가장 오래된 페이지를 파기하고 가장 새로운 것을 남겨놓게 된다.
* DB 도 계속 구동시키면 캐시가 점점 최적화되어 가므로 기동시킨 직후보다 뒤로 갈수록 부하, I/O 가 내려가는 특성을 보인다.

- - -
## 강의 9 - I/O 부하를 줄이는 방법
* 데이터 규모에 비해 물리 메모리가 크면 전부 캐싱할 수 있으므로 이 점을 고려하면 좋다.
  * (단순히 비용 문제만 극복할 수 있다면, 단순히 물리 메모리를 늘려주기만 해도 충분한가?)
* 대규모 데이터에 대해서 압축을 해서 저장해두면 디스크 내용을 더 많이 캐싱할 수 있다.
  * 압축된 데이터를 외부로 전송할 때 네트워크 I/O 또한 경감시킬 수 있는 이점이 있다. 
  * 다만 압축하고, 해제하는 비용으로 인해 CPU 사용률이 올라갈 것이다.

### 복수 서버로 확장시키기
* 데이터 규모가 점점 커지다보면 메모리에 전부 캐싱할 수 없는 순간이 오는데, 복수 서버로 확장시키는 방법을 생각해볼 수 있다.
* 애플리케이션 서버를 수평확장 할 때는 CPU 부하를 분산시키기 위함이지만, DB 서버의 경우에는 부하 분산뿐만 아니라 캐시 용량을 늘리고자 할 때 혹은 효율을 높이고자 할 때인 경우가 많다.

### 단순히 대수만 늘려서는 확장성을 확보할 수 없다
* DB 서버는 단순하게 늘리기만 한다고 해서 마냥 좋은 것은 아니다.
  * 데이터를 복사해서 대수를 늘리게 되면 애초에 캐시 용량이 부족해서 늘렸는데 그 부족한 부분도 그대로 동일하게 늘려가게 되기 때문이다.
