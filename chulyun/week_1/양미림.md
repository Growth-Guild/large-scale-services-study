1주차 (p.28 ~ p.78)
- - -

# chapter 2 대규모 데이터 처리 입문(메모리와 디스크, 웹 애플리케이션과 부하)

## 강의 4. 하테나 북마크의 데이터 규모
* 웹 어플리케이션에서의 대규모 서비스의 규모 기준 엔트리3GB, 북마크 5.5GB, 태그 4.8GB, HTML: 2000GB 이상 
* 대규모 데이터를 쿼리하기 위해서는 인덱스를 잘 태워야한다. 
* 시행착오를 찾아가는 것 또한 경험이므로 문제가 발생하면 그 자리에서 생각하자는 느낌으로 공부해가는 것이 좋다. 

## 강의 5. 대규모 데이터 처리의 어려운 점 
### 대규모 데이터는 메모리 내에서 계산 할 수 없다. 
* 메모리에 올리지 않으면 기본적으로 디스크를 계속 읽어야한다
* 디스크를 읽을 것는 메모리를 읽는것 보다 느리다 I/O에 시간이 오래걸린다
* 어떻게 대처해야하는가? 서비스에 맞춰서 항상 고민해야한다. 
### 메모리와 디스크는 10^5~10^6배 속도 차가 있다 (메모리>디스크) 
* 메모리는 디스크보다 10^5~10^6 빠르다. 
### 디스크는 왜 늦을까? 메모리와 디스크
* 메모리는 전기적 부품으로 마이크로초 단위로 포인터를 이동시킬수 있어서 탐색이 빠르다.
* 디스크는 물리적 부품으로 동축상 원반이 쌓여있는데 이것이 회전하므로써 데이터를 탐색하게 된다. 
* 디스크 탐색속도에 영향을 주는 요인으로는 디스크 헤드의 이동과 원반의 회전이라는 물리적이동이다. 
### OS레벨에서 연구
* 데이터를 탐색할때 이곳저곳에 흩어져 있으면 원반과 헤드가 많이 움직여 속도가 느려진다. 
* OS는 데이터를 적재할 때 연속된 데이터를 같은 위치에 쌓으므로써 회전수를 최소화한다. 
* 그래도 메모리가 더 빠르다
### 전송속도, 버스의 속도차 
* 탐색속도는 10^5~16^6배, 전송속도는 100배 차이남 (메모리 7.5GB >디스크 58MB)
* Timing Cached Reads 메모리 캐시 데이터 전송속도
* Timing bufferd disk reads 디스크의 전송속도 

### Linux 단일 호스트의 부하 
* 계측을 총하여 단일 호스트의 성능을 끌어낸다. (서버를 분산하는건 좋은 해결책이아님)
* 병목 작업을 규명해야한다. (Load Average, CPU, I/O 병목현상확인 필요)
1. Load Average 확인 top, uptime 명령으로 부하판단, 병목지점 조사
2. CPU, I/O 부하 확인 sar, vmstat로 시간대비 CPU사용률, I/O대기율 추이 확인
3. CPU 과부하의 경우 사용자 프로그램인지 시스템 프로그램인지 top, sar로 확인 -> 프로그램 로직이나 알고리즘 개선으로 해결
4. ps로 프로세스 상태나 cpu사용시간을 본다
5. strace, oprofile로 대상 프로세스를 세부조사하여 병목을 해결한다.
* 시스템 전송량 자체에 문제가 있다면 서버증설
6. I/O 부하가 높은 경우 프로그램으로부터 입출력 자체가 많아서 스왑이 발생해서 디스크 액세스가 발생하고 있을 수 있음.sar wmstat를 통해 문제 발견
7. ps를 통해 극단적으로 메모리를 소비하고 있지 않은지 확인
8. 메모리 부족의 경우 메모리 증설을 고려하고 그렇지 못할 상황에는 메모리 분산을 검토한다. 
9. 스왑이 발생하지 않았는데 메모리가 부족한 경우는 서버용량과 증설 가능한 메모리 양을 검토한다.
10. 메모리증설->캐시영역확대가 가능한지 확인
11. 데이터 분산이나 캐시서버 도입을 검토
12. I/O의 빈도 자체를 줄이는 것을 검토
* OS 튜닝이란 부하의 원인을 알고 제거하는 것이지 물리적 성능을 뛰어넘는 것이 아니다. 다음과 같은 것을 체크하고 의사결정을 한다.
1. 메모리 증설로 캐시 영역을 확보하는 것으로 해결이 가능한지 
2. 데이터량이 너무 많은 것이 아닌지
3. 애플리케이션 측의 I/O 알고리즘을 변경할 필요가 없는지? 

## 강의 6. 규모조정의 요소
### 규모조정(Scaling), 확장성(scalability)
* Scale-Up:고가의 장비를 사서 처리성능을 높이는것 Scale-out: 비슷한 장비를 나열해서 전체 성능을 높이는것(스케일아웃쪽이 가성비좋음)
### 규모조정의 요소 
* CPU부하, I/O부하가 있고 스케일업 혹은 스케일아웃이 어떤것이 적합할지 연구 해야함. 
* 보통 CPU부하는 스케일 아웃을 통해 로드밸런스로 분산하거나 웹,AP서버,크롤러 등으로 나누면 된다. 
* 반면 I/O부하의 경우 DB 확장성을 확보하기 어려움이 있음. 
### 웹 애플리케이션과 부하의 관계
* 웹 애플리케이션 3단구조 (프록시-AP서버-DB) 
* 같은 서버라도 그 역할에 따라 부하의 종류가 다르게 되고 확작 할 수 있는지 없는지를 판단할 수 있다.


## 강의 7. 대규모 데이터를 다루기 위한 기초지식 

### 대규모 데이터를 다루는 세가지 급소 - 프로그램을 작성할 때의 요령
1. 어떻게 하면 메모리에서 처리를 마칠수 있을까? 디스크seek횟수 최소화, 국고성을 활용한 분산 실현
2. 데이터량 증가에 강한 알고리즘을 사용하는것, 선형검색->이분검색 등
3. 데이터 압축이나 검색기술과 같은 테크닉을 활용한다.  
### 대규모 데이터를 다루기 전 3대 전제지식 - 프로그램 개발의 한층 아래 기초
1. OS캐시
2. 분산을 고려한 RDBMS 운용
3. 알고리즘과 데이터 구조

### CPU 사용율과 I/O 대기율
* sar명령어로 CPU바운드의 경우 system의 사용율을 보고 판단한다, 수치가 높다면 CPU리소스 부족이라 판단할 수 있음
* sar명령어로 I/O바운드의 경우 oiwait보면 대기율을 보고 판단한다. 
* 듀얼코어 CPU의 경우 -p 옵션으로 개별적으로 살펴야함.

- - -

# chapter 3. OS캐시와 분산(대규모 데이터를 효율적으로 처리하는 원리)
* I/O 대책에 대한 기반은 OS에 있음. 
* OS캐시와 분산
* 캐시를 전재로 한 I/O 부하 줄이는 방법
* 캐시를 고려한 국소성을 살리는 분산

## 강의 8. OS의 캐시구조 - 대규모 데이터를 효율적으로 처리하는 원리
### OS의 캐시 구조를 알고 애플리케이션 작성하기 - 페이지 캐시
* OS캐시 - OS가 메모리를 이용해서 디스크 액세스를 줄이는것.
* 메모리, 디스크, OS 캐시 구조 
* Linux의 경우 page cache, file cache, buffer cache라고 하는 캐시 구조를 갖추고 있다.
* Linux(x86)의 페이징 구조(가상 메모리 구조 : 선형 어드레스->페이징구조->물리 어드레스) 
* 스왑 : 가상메모리를 응용한 기능중 하나로 물리메모리가 부족할 때 2차 기억장치(주로 디스크)를 메모리로 간주해서 외형상의 메모리 부족을 해소하는 원리.
### 가상 메모리 구조 
* 프로세스에서 메모리를 다루기 쉽게 하는 이점을 제공한다
* OS가 커널 내에서 메모리를 추상화 하고있다
* 페이지: OS가 물리 메모리를 확보/관리하는 단위

1. 디스크의 내용을 일단 메모리에 읽어들인다->페이지가 작성된다
2. 작성된 페이지는 파기되지 않고 남는다-> 페이지 캐시
3. 예외의 경우를 제외하고 모든 I/O에 투과적으로 작용한다 -> 디스크의 캐시를 담당하는곳(VFS)
* VFS란..?! 
* 디바이스 드라이버와 OS사이에 끼여있는 파일시스템위에 이들간의 인터페이스를 통일하는 VFS 추상화 레이어가 있다. 
### Linux는 페이지 단위로 디스크를 캐싱한다
* 페이지 = 가상 메모리의 최소 단위 
* LRU(Least Recently used)
* 어떤파일의 어느위치를 쌍으로 캐시의 키를 관리할 수 있는 특징 때문에 일부만 캐싱해갈 수 있다.
* OS(=커널)내부에서 사용되고 있는 데이터 구조는 Radix Tree라고 하며, 파일이 아무리 커지더라도 캐시 탐색 속도가 떨어지지 않도록 개발된 데이터 구조이다. 
### 메모리가 비어 있으면 캐싱 - sar로 확인해보기
### 메모리를 늘려서 I/O부하 줄이기  
### 페이지 캐시는 투과적으로 작용한다. 
### sar 명령으로 OS가 보고하는 각종 지표 참조하기
* sar -u CPU사용률 확인
* sar -q Load Average 확인
* sar -r 메모리 사용 현황 확인
* sar -W 스왑발생

## 강의 9. I/O 부하를 줄이는 방법
### 캐시를 전제로 한 I/O 줄이는 방법
* 데이터규모 < 물리메모리 이면 전부 캐싱할 수 있다.
* 경제적 비용과의 밸런스 고려-> 현재 일반적인 서버 메모리는 8GB~16GB
### 복수 서버로 확장시키지 - 캐시로 해결될 수 없는 규모일 경우
* CPU 부하분산에는 단순히 늘린다
* I/O 분산에는 국소성을 고려한다
### 단순히 대수만 늘려서는 확장성을 확보할 수 없다. 
* 캐싱할 수 없는 비율은 변함없이 그대로-> 곧 다시 병목된다. 
